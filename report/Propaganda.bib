@article{amerCriticalDiscourseAnalysis2017,
  title = {Critical Discourse Analysis of War Reporting in the International Press: The Case of the {{Gaza}} War of 2008–2009},
  shorttitle = {Critical Discourse Analysis of War Reporting in the International Press},
  author = {Amer, Mohammedwesam},
  year = {2017},
  month = oct,
  journal = {Palgrave Commun},
  volume = {3},
  number = {1},
  pages = {13},
  issn = {2055-1045},
  doi = {10.1057/s41599-017-0015-2},
  url = {https://www.nature.com/articles/s41599-017-0015-2},
  urldate = {2024-05-13},
  abstract = {Abstract             This paper employs critical discourse analysis (CDA) to analyse the representation of political social actors in media coverage of the Gaza war of 2008–2009. The paper examines texts of systematically chosen news stories from four international newspapers: ‘The Guardian, The Times London, The New York Times and The Washington Post’. The findings show substantial similarities in representation patterns among the four newspapers. More specifically, the selected newspapers foreground Israeli agency in achieving a ceasefire, whereby Israeli actors are predominantly assigned activated roles. By contrast, the four newspapers foreground Palestinian agency in refusing ceasefire through assigning activated roles. The findings of this study suggest that news reports on the Gaza war of 2008–2009 are influenced by the political orientations of the newspapers and also their liberal and conservative ideological stances. Overall, the most represented actors are Israeli governmental officials, whereas Palestinian actors are Hamas members. This representation draws an overall image that the war is being directed against Hamas.},
  langid = {english},
  file = {/Users/debbs/Library/Mobile Documents/com~apple~CloudDocs/Documents/Documents - Debb's MacBook Pro/School Work/Zotero-PDFs/Amer_2017_Critical discourse analysis of war reporting in the international press.pdf}
}

@article{chawlaSMOTESyntheticMinority2002,
  title = {{{SMOTE}}: {{Synthetic Minority Over-sampling Technique}}},
  shorttitle = {{{SMOTE}}},
  author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
  year = {2002},
  month = jun,
  journal = {jair},
  volume = {16},
  eprint = {1106.1813},
  primaryclass = {cs},
  pages = {321--357},
  issn = {1076-9757},
  doi = {10.1613/jair.953},
  url = {http://arxiv.org/abs/1106.1813},
  urldate = {2024-05-13},
  abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/debbs/Library/Mobile Documents/com~apple~CloudDocs/Documents/Documents - Debb's MacBook Pro/School Work/Zotero-PDFs/Chawla_2002_SMOTE.pdf;/Users/debbs/Zotero/storage/KPA3DUTP/1106.html}
}

@article{cortesSupportvectorNetworks1995,
  title = {Support-Vector Networks},
  author = {Cortes, Corinna and Vapnik, Vladimir},
  year = {1995},
  month = sep,
  journal = {Mach Learn},
  volume = {20},
  number = {3},
  pages = {273--297},
  issn = {1573-0565},
  doi = {10.1007/BF00994018},
  url = {https://doi.org/10.1007/BF00994018},
  urldate = {2024-05-13},
  abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
  langid = {english},
  keywords = {efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
  file = {/Users/debbs/Library/Mobile Documents/com~apple~CloudDocs/Documents/Documents - Debb's MacBook Pro/School Work/Zotero-PDFs/Cortes_1995_Support-vector networks.pdf}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  year = {2019},
  month = jun,
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1423},
  url = {https://aclanthology.org/N19-1423},
  urldate = {2024-05-13},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  file = {/Users/debbs/Library/Mobile Documents/com~apple~CloudDocs/Documents/Documents - Debb's MacBook Pro/School Work/Zotero-PDFs/Devlin_2019_BERT.pdf}
}

@article{lopatinClashofcivilizationsPrismGerman2017,
  title = {A Clash-of-Civilizations Prism in {{German}} Media? {{Documenting}} a Shift from Political to Religious Framing of the {{Israeli}}–{{Palestinian}} Conflict},
  shorttitle = {A Clash-of-Civilizations Prism in {{German}} Media?},
  author = {Lopatin, Esther and {Samuel-Azran}, Tal and Galily, Yair},
  year = {2017},
  month = mar,
  journal = {Communication and the Public},
  volume = {2},
  number = {1},
  pages = {19--34},
  publisher = {SAGE Publications},
  issn = {2057-0473},
  doi = {10.1177/2057047316689795},
  url = {https://doi.org/10.1177/2057047316689795},
  urldate = {2024-05-13},
  abstract = {The impact of political events on media’s conflict coverage prism is widely established. To assess the role of mounting tensions between Muslims and non-Muslims in Europe in 2013–2014 on German media’s coverage prism of Muslim-related conflicts, this article compares coverage of the 2008 and 2014 Israeli military operations in Gaza by three major German newspapers. The empirical analysis indicates a dramatic rise in the use of religious terms in 2014, most notably in conservative newspaper Die Welt, and offers evidence of a shift from politics-centered framing of the 2008 Gaza operation to a more religious-centered framing of the 2014 Gaza War. We discuss wider implications of the findings, including their support for the relevance of the clash-of-civilizations theory to contemporary media’s conflict coverage modus operandi.},
  langid = {english},
  file = {/Users/debbs/Library/Mobile Documents/com~apple~CloudDocs/Documents/Documents - Debb's MacBook Pro/School Work/Zotero-PDFs/Lopatin_2017_A clash-of-civilizations prism in German media.pdf}
}

@misc{michaelSleepingLastPluto2017,
  title = {Sleeping at {{Last}} - {{Pluto}} ({{Instrumental}})},
  author = {{Michael}},
  year = {2017},
  month = feb,
  url = {https://www.youtube.com/watch?v=LX0C88JLCPQ},
  urldate = {2024-06-30},
  abstract = {Go and support Sleeping at Last by buying his Space (Deluxe) album: https://itunes.apple.com/album/atlas-...}
}

@misc{mikolovEfficientEstimationWord2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  number = {arXiv:1301.3781},
  eprint = {1301.3781},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1301.3781},
  url = {http://arxiv.org/abs/1301.3781},
  urldate = {2024-05-13},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/debbs/Library/Mobile Documents/com~apple~CloudDocs/Documents/Documents - Debb's MacBook Pro/School Work/Zotero-PDFs/Mikolov_2013_Efficient Estimation of Word Representations in Vector Space.pdf;/Users/debbs/Zotero/storage/LXA5YQRX/1301.html}
}

@inproceedings{penningtonGloveGlobalVectors2014,
  title = {Glove: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {Glove},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  pages = {1532--1543},
  publisher = {Association for Computational Linguistics},
  address = {Doha, Qatar},
  doi = {10.3115/v1/D14-1162},
  url = {http://aclweb.org/anthology/D14-1162},
  urldate = {2024-05-13},
  abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
  langid = {english}
}

@misc{powersEvaluationPrecisionRecall2020,
  title = {Evaluation: From Precision, Recall and {{F-measure}} to {{ROC}}, Informedness, Markedness and Correlation},
  shorttitle = {Evaluation},
  author = {Powers, David M. W.},
  year = {2020},
  month = oct,
  number = {arXiv:2010.16061},
  eprint = {2010.16061},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.16061},
  url = {http://arxiv.org/abs/2010.16061},
  urldate = {2024-05-13},
  abstract = {Commonly used evaluation measures including Recall, Precision, F-Measure and Rand Accuracy are biased and should not be used without clear understanding of the biases, and corresponding identification of chance or base case levels of the statistic. Using these measures a system that performs worse in the objective sense of Informedness, can appear to perform better under any of these commonly used measures. We discuss several concepts and measures that reflect the probability that prediction is informed versus chance. Informedness and introduce Markedness as a dual measure for the probability that prediction is marked versus chance. Finally we demonstrate elegant connections between the concepts of Informedness, Markedness, Correlation and Significance as well as their intuitive relationships with Recall and Precision, and outline the extension from the dichotomous case to the general multi-class case.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/debbs/Library/Mobile Documents/com~apple~CloudDocs/Documents/Documents - Debb's MacBook Pro/School Work/Zotero-PDFs/Powers_2020_Evaluation.pdf;/Users/debbs/Zotero/storage/GVHAVRTG/Powers - 2020 - Evaluation from precision, recall and F-measure t.pdf;/Users/debbs/Zotero/storage/H9EVQ6MF/2010.html}
}

@article{skafleMisinformationCOVID19Vaccines2022a,
  title = {Misinformation {{About COVID-19 Vaccines}} on {{Social Media}}: {{Rapid Review}}},
  shorttitle = {Misinformation {{About COVID-19 Vaccines}} on {{Social Media}}},
  author = {Skafle, Ingjerd and {Nordahl-Hansen}, Anders and Quintana, Daniel S. and Wynn, Rolf and Gabarron, Elia},
  year = {2022},
  month = aug,
  journal = {Journal of Medical Internet Research},
  volume = {24},
  number = {8},
  pages = {e37367},
  publisher = {JMIR Publications Inc., Toronto, Canada},
  doi = {10.2196/37367},
  url = {https://www.jmir.org/2022/8/e37367},
  urldate = {2024-05-13},
  abstract = {Background: The development of COVID-19 vaccines has been crucial in fighting the pandemic. However, misinformation about the COVID-19 pandemic and vaccines is spread on social media platforms at a rate that has made the World Health Organization coin the phrase infodemic. False claims about adverse vaccine side effects, such as vaccines being the cause of autism, were already considered a threat to global health before the outbreak of COVID-19. Objective: We aimed to synthesize the existing research on misinformation about COVID-19 vaccines spread on social media platforms and its effects. The secondary aim was to gain insight and gather knowledge about whether misinformation about autism and COVID-19 vaccines is being spread on social media platforms. Methods: We performed a literature search on September 9, 2021, and searched PubMed, PsycINFO, ERIC, EMBASE, Cochrane Library, and the Cochrane COVID-19 Study Register. We included publications in peer-reviewed journals that fulfilled the following criteria: original empirical studies, studies that assessed social media and misinformation, and studies about COVID-19 vaccines. Thematic analysis was used to identify the patterns (themes) of misinformation. Narrative qualitative synthesis was undertaken with the guidance of the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) 2020 Statement and the Synthesis Without Meta-analysis reporting guideline. The risk of bias was assessed using the Joanna Briggs Institute Critical Appraisal tool. Ratings of the certainty of evidence were based on recommendations from the Grading of Recommendations Assessment, Development and Evaluation Working Group. Results: The search yielded 757 records, with 45 articles selected for this review. We identified 3 main themes of misinformation: medical misinformation, vaccine development, and conspiracies. Twitter was the most studied social media platform, followed by Facebook, YouTube, and Instagram. A vast majority of studies were from industrialized Western countries. We identified 19 studies in which the effect of social media misinformation on vaccine hesitancy was measured or discussed. These studies implied that the misinformation spread on social media had a negative effect on vaccine hesitancy and uptake. Only 1 study contained misinformation about autism as a side effect of COVID-19 vaccines. Conclusions: To prevent these misconceptions from taking hold, health authorities should openly address and discuss these false claims with both cultural and religious awareness in mind. Our review showed that there is a need to examine the effect of social media misinformation on vaccine hesitancy with a more robust experimental design. Furthermore, this review also demonstrated that more studies are needed from the Global South and on social media platforms other than the major platforms such as Twitter and Facebook. Trial Registration: PROSPERO International Prospective Register of Systematic Reviews CRD42021277524; https://www.crd.york.ac.uk/prospero/display\_record.php?ID=CRD42021277524},
  langid = {english},
  file = {/Users/debbs/Library/Mobile Documents/com~apple~CloudDocs/Documents/Documents - Debb's MacBook Pro/School Work/Zotero-PDFs/Skafle_2022_Misinformation About COVID-19 Vaccines on Social Media2.pdf}
}

@article{sokolovaSystematicAnalysisPerformance2009,
  title = {A Systematic Analysis of Performance Measures for Classification Tasks},
  author = {Sokolova, Marina and Lapalme, Guy},
  year = {2009},
  month = jul,
  journal = {Information Processing \& Management},
  volume = {45},
  number = {4},
  pages = {427--437},
  issn = {03064573},
  doi = {10.1016/j.ipm.2009.03.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457309000259},
  urldate = {2024-05-13},
  abstract = {Semantic Scholar extracted view of "A systematic analysis of performance measures for classification tasks" by Marina Sokolova et al.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english}
}
